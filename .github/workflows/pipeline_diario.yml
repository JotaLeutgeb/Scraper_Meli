# .github/workflows/pipeline_diario.yml

name: Radar de Competencia - Ejecución Diaria

# --- TRIGGERS: ¿Cuándo debe ejecutarse este workflow? ---
on:
  # 1. Ejecución programada (schedule) usando sintaxis de Cron.
  # "0 3 * * *" significa "a las 3:00 AM UTC todos los días".
  # Ajusta la hora según la necesidad estratégica del negocio.
  schedule:
    - cron: '0 9 * * *'

  # 2. Ejecución manual (workflow_dispatch)
  # Esto añade un botón "Run workflow" en la pestaña "Actions" de GitHub.
  # Esencial para pruebas, depuración y ejecuciones bajo demanda.
  workflow_dispatch:

# --- JOBS: ¿Qué tareas debe realizar? ---
jobs:
  # Definimos un único job llamado "run-pipeline"
  run-pipeline:
    # El tipo de máquina virtual donde se ejecutará. "ubuntu-latest" es estándar y eficiente.
    runs-on: ubuntu-latest
    
    # Los pasos que componen el job se ejecutan en secuencia.
    steps:
      # Paso 1: Descargar el código del repositorio
      # Es el primer paso estándar en casi cualquier workflow.
      - name: 1. Checkout del código del repositorio
        uses: actions/checkout@v4

      # Paso 2: Configurar el entorno de Python
      # Especificamos la versión de Python que queremos usar.
      - name: 2. Configurar Python 3.13.2
        uses: actions/setup-python@v5
        with:
          python-version: '3.13.2'
          cache: 'pip' # Activa el cache para acelerar instalaciones futuras

      # Paso 3: Instalar dependencias de Python
      # Lee nuestro requirements.txt y las instala usando pip.
      - name: 3. Instalar dependencias de Python
        run: pip install -r requirements.txt

      # Paso 4: Instalar los navegadores para Playwright
      # Este paso es CRÍTICO para que el scraper funcione.
      # El flag --with-deps instala también las dependencias del sistema operativo.
      - name: 4. Instalar navegadores de Playwright
        run: playwright install --with-deps chromium

      # Paso 5: Ejecutar el pipeline principal
      # Aquí es donde llamamos a nuestro script main.py.
      # La sección "env" es la clave: mapea los GitHub Secrets a las variables de entorno
      # que el script espera encontrar.
      - name: 5. Ejecutar el Pipeline de Datos
        run: python main.py
        env:
          DB_HOST: ${{ secrets.DB_HOST }}
          DB_PORT: ${{ secrets.DB_PORT }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USER: ${{ secrets.DB_USER }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}